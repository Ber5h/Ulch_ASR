{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBSYoWbi-45k"
      },
      "source": [
        "# **Fine-Tune W2V2-Bert for low-resource ASR with ü§ó Transformers**\n",
        "\n",
        "***New (01/2024)***: *This blog post is strongly inspired by \"[Fine-tuning XLS-R on Multi-Lingual ASR](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2)\"* and [\"Fine-tuning MMS Adapter Models for Multi-Lingual ASR\"](https://huggingface.co/blog/mms_adapters)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nT_QrfWtsxIz"
      },
      "source": [
        "## Notebook Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YELVqGxMxnbG"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8eh87Hoee5d"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install torchaudio\n",
        "!pip install jiwer\n",
        "!pip install accelerate -U\n",
        "!pip install ipywidgets\n",
        "!pip install soundfile\n",
        "!pip install huggingface_hub\n",
        "!pip install librosa\n",
        "!pip install soundfile\n",
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSRGc4WRdEYU"
      },
      "outputs": [],
      "source": [
        "import IPython.display as ipd\n",
        "import numpy as np\n",
        "import random\n",
        "from datasets import ClassLabel\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "import torch\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Any, Dict, List, Optional, Union\n",
        "import soundfile as sf\n",
        "import warnings\n",
        "from evaluate import load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlMSH3T3EazV"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(token='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')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcR-d83OEkqb"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!apt install git-lfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDvVc6wWQ6C7"
      },
      "outputs": [],
      "source": [
        "!apt-get update\n",
        "!apt install libsndfile1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mW-C1Nt-j7k"
      },
      "source": [
        "## Prepare Data, Tokenizer, Feature Extractor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEXEWEJGQPqD"
      },
      "source": [
        "### Create `Wav2Vec2CTCTokenizer`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89zsaff5JhYh"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, Audio\n",
        "import datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8x_DD-xwJzB"
      },
      "outputs": [],
      "source": [
        "datasets.config.DOWNLOADER_TIMEOUT = 10000  # –£–≤–µ–ª–∏—á–µ–Ω–∏–µ —Ç–∞–π–º–∞—É—Ç–∞ –¥–æ 120 —Å–µ–∫—É–Ω–¥\n",
        "datasets.config.DOWNLOADER_MAX_RETRIES = 100  # –ü–æ–≤—Ç–æ—Ä–Ω—ã–µ –ø–æ–ø—ã—Ç–∫–∏\n",
        "\n",
        "dataset = load_dataset(\"Ber5h/Ulch_ASR_dataset\", split=\"train\").train_test_split(test_size=0.2, seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzJCMbEVPX_y"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNmlZ9NF4N3K"
      },
      "outputs": [],
      "source": [
        "ulch_train = dataset['train']\n",
        "ulch_test = dataset['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfzp4IeR8lYZ"
      },
      "outputs": [],
      "source": [
        "ulch_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Go9Hq4e4NDT9"
      },
      "source": [
        "Let's write a short function to display some random samples of the dataset and run it a couple of times to get a feeling for the transcriptions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72737oog2F6U"
      },
      "outputs": [],
      "source": [
        "def show_random_elements(dataset, num_examples=10):\n",
        "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
        "    picks = []\n",
        "    for _ in range(num_examples):\n",
        "        pick = random.randint(0, len(dataset)-1)\n",
        "        while pick in picks:\n",
        "            pick = random.randint(0, len(dataset)-1)\n",
        "        picks.append(pick)\n",
        "\n",
        "    df = pd.DataFrame(dataset[picks])\n",
        "    display(HTML(df.to_html()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntKkq7jRKf9-"
      },
      "outputs": [],
      "source": [
        "def dummy_npwarn_decorator_factory():\n",
        "  def npwarn_decorator(x):\n",
        "    return x\n",
        "  return npwarn_decorator\n",
        "np._no_nep50_warning = getattr(np, '_no_nep50_warning', dummy_npwarn_decorator_factory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3ml7PYwKf9_"
      },
      "outputs": [],
      "source": [
        "np.version.version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_JUmf3G3b9S"
      },
      "outputs": [],
      "source": [
        "show_random_elements(ulch_train, num_examples=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vq7OR50LN49m"
      },
      "source": [
        "We can see that the transcriptions contain some special characters, such as `,.?!;:`. Without a language model, it is much harder to classify speech chunks to such special characters because they don't really correspond to a characteristic sound unit. *E.g.*, the letter `\"s\"` has a more or less clear sound, whereas the special character `\".\"` does not.\n",
        "Also in order to understand the meaning of a speech signal, it is usually not necessary to include special characters in the transcription.\n",
        "\n",
        "Let's simply remove all characters that don't contribute to the meaning of a word and cannot really be represented by an acoustic sound and normalize the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svKzVJ_hQGK6"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "chars_to_remove_regex = '[,?.!-;:\"‚Äú%‚Äò‚Äù\\'¬ª\\¬´\\‚Äû\\‚Äê\\‚Äì()=[]\\\\]'\n",
        "\n",
        "def remove_special_characters(batch):\n",
        "    # remove special characters\n",
        "    batch[\"transcription\"] = re.sub(chars_to_remove_regex, '', batch[\"transcription\"]).strip().lower()\n",
        "\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIHocAuTQbBR"
      },
      "outputs": [],
      "source": [
        "ulch_train = ulch_train.map(remove_special_characters)\n",
        "ulch_test = ulch_test.map(remove_special_characters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxnVS9gIhIma"
      },
      "source": [
        "Let's look at the processed text labels again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBDRAAYxRE6n"
      },
      "outputs": [],
      "source": [
        "show_random_elements(ulch_train.remove_columns([\"audio\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ORHDb2Th2TW"
      },
      "source": [
        "In CTC, it is common to classify speech chunks into letters, so we will do the same here.\n",
        "Let's extract all distinct letters of the training and test data and build our vocabulary from this set of letters.\n",
        "\n",
        "We write a mapping function that concatenates all transcriptions into one long transcription and then transforms the string into a set of chars.\n",
        "It is important to pass the argument `batched=True` to the `map(...)` function so that the mapping function has access to all transcriptions at once."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFPGfet8U5sL"
      },
      "source": [
        "Cool, now our vocabulary is complete and consists of 36 tokens, which means that the linear layer that we will add on top of the pretrained W2V-BERT checkpoint will have an output dimension of 36."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CujRgBNVRaD"
      },
      "source": [
        "Let's now save the vocabulary as a json file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0weDGLZJOOB8"
      },
      "outputs": [],
      "source": [
        "vocab_dict = {\n",
        "  \"[PAD]\": 51,\n",
        "  \"[UNK]\": 52,\n",
        "  '·ª•':0,\n",
        " 'a': 1,\n",
        " 'b': 2,\n",
        " \"–±\": 3,\n",
        " 'c': 4,\n",
        " 'd': 5,\n",
        " \"–¥\": 6,\n",
        " 'e': 7,\n",
        " 'f': 8,\n",
        " \"—Ñ\": 9,\n",
        " 'g': 10,\n",
        " \"–≥\": 11,\n",
        " 'i': 12,\n",
        " 'j': 13,\n",
        " 'k': 14,\n",
        " \"\": 15,\n",
        " 'l': 16,\n",
        " \"–ª\": 17,\n",
        " 'm': 18,\n",
        " \"–º\":19,\n",
        " 'n': 20,\n",
        " 'o': 21,\n",
        " 'p' : 22,\n",
        " \"–ø\": 23,\n",
        " 'r': 24,\n",
        " \"—Ä\": 25,\n",
        " 's': 26,\n",
        " \"—Å\": 27,\n",
        " 't': 28,\n",
        " \"—Ç\": 29,\n",
        " 'u': 30,\n",
        " 'v': 31,\n",
        " \"–≤\": 32,\n",
        " 'w': 33,\n",
        " 'x': 34,\n",
        " 'z': 35,\n",
        " \"–∑\": 36,\n",
        " 'ƒÅ': 37,\n",
        " 'ƒç': 38,\n",
        " 'ƒì': 39,\n",
        " 'ƒ´': 40,\n",
        " '≈Ñ': 41,\n",
        " '≈ã': 42,\n",
        " '≈ç': 43,\n",
        " '≈°': 44,\n",
        " \"—â\": 45,\n",
        " '≈´': 46,\n",
        " '«Ø': 47,\n",
        " '…ô': 48,\n",
        " '…£': 49,\n",
        " '…ôÃÑ': 50\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehyUoh9vk191"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "with open('vocab.json', 'w') as vocab_file:\n",
        "    json.dump(vocab_dict, vocab_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHJDaKlIVVim"
      },
      "source": [
        "In a final step, we use the json file to load the vocabulary into an instance of the `Wav2Vec2CTCTokenizer` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xriFGEWQkO4M"
      },
      "outputs": [],
      "source": [
        "from transformers import Wav2Vec2CTCTokenizer\n",
        "\n",
        "tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(\"./\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1XApZBAF2zr"
      },
      "outputs": [],
      "source": [
        "repo_name = \"wav2vec-bert-2.0-ulch-try\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1nkdOFRKf-E"
      },
      "outputs": [],
      "source": [
        "tokenizer.push_to_hub(repo_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1BiezWZF16d"
      },
      "source": [
        "and upload the tokenizer to the [ü§ó Hub](https://huggingface.co/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYcIiR2FQ96i"
      },
      "source": [
        "### Create `SeamlessM4TFeatureExtractor`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuUbPW7oV-B5"
      },
      "source": [
        "The role of the `SeamlessM4TFeatureExtractor` is to prepare the raw audio input in a format that the model can \"understand\". It therefore maps the sequence of one-dimensional amplitude values (aka the raw audio input) to a two-dimensional matrix of log-mel spectrogram values. The latter encodes the signal frequency information as a function of time. See [this section](https://huggingface.co/learn/audio-course/chapter1/audio_data#the-frequency-spectrum) from the Audio Transformers course to learn more about spectrograms and why they are important.\n",
        "\n",
        "Unlike the tokenizer, the feature extractor doesn't need to be \"learned\" from the data, so we can load it directly from the [initial model checkpoint](https://huggingface.co/facebook/w2v-bert-2.0).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAR0-2KLkopp"
      },
      "outputs": [],
      "source": [
        "from transformers import SeamlessM4TFeatureExtractor\n",
        "\n",
        "feature_extractor = SeamlessM4TFeatureExtractor(feature_size=80, num_mel_bins=80, sampling_rate=16000, padding_value=0.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUETetgqYC3W"
      },
      "source": [
        "Great, W2V-BERT's feature extraction pipeline is thereby fully defined!\n",
        "\n",
        "For improved user-friendliness, the feature extractor and tokenizer are *wrapped* into a single `Wav2Vec2BertProcessor` class so that one only needs a `model` and `processor` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYZtoW-tlZgl"
      },
      "outputs": [],
      "source": [
        "from transformers import Wav2Vec2BertProcessor\n",
        "\n",
        "processor = Wav2Vec2BertProcessor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrKnYuvDIoOO"
      },
      "source": [
        "Next, we can prepare the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFmShnl7RE35"
      },
      "source": [
        "### Preprocess Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTCS7W6XJ9BG"
      },
      "outputs": [],
      "source": [
        "ulch_train[0][\"audio\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6ndIjHGFp0W"
      },
      "source": [
        "W2V-BERT expects the input in the format of a 1-dimensional array of 16 kHz. This means that the audio file has to be loaded and resampled.\n",
        "\n",
        " Thankfully, `datasets` does this automatically by calling the other column `audio`. Let try it out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrv65aj7G95i"
      },
      "outputs": [],
      "source": [
        "ulch_train = ulch_train.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
        "ulch_test = ulch_test.cast_column(\"audio\", Audio(sampling_rate=16_000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcnO4x-NGBEi"
      },
      "source": [
        "Let's take a look at `\"audio\"` again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKtkc1o_HWHC"
      },
      "outputs": [],
      "source": [
        "ulch_train[0][\"audio\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOckzFd4Mbzq"
      },
      "source": [
        "This seemed to have worked! Let's listen to a couple of audio files to better understand the dataset and verify that the audio was correctly loaded.\n",
        "\n",
        "**Note**: *You can click the following cell a couple of times to listen to different speech samples.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dueM6U7Ev0OA"
      },
      "outputs": [],
      "source": [
        "rand_int = random.randint(0, len(ulch_train)-1)\n",
        "\n",
        "print(ulch_train[rand_int][\"transcription\"])\n",
        "ipd.Audio(data=ulch_train[rand_int][\"audio\"][\"array\"], autoplay=True, rate=16000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gY8m3vARHYTa"
      },
      "source": [
        "It seems like the data is now correctly loaded and resampled."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MaL9J2dNVtG"
      },
      "source": [
        "It can be heard, that the speakers change along with their speaking rate, accent, and background environment, etc. Overall, the recordings sound acceptably clear though, which is to be expected from a crowd-sourced read speech corpus.\n",
        "\n",
        "Let's do a final check that the data is correctly prepared, by printing the shape of the speech input, its transcription, and the corresponding sampling rate.\n",
        "\n",
        "**Note**: *You can click the following cell a couple of times to verify multiple samples.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Po2g7YPuRTx"
      },
      "outputs": [],
      "source": [
        "rand_int = random.randint(0, len(ulch_train)-1)\n",
        "\n",
        "print(\"Target text:\", ulch_train[rand_int][\"transcription\"])\n",
        "print(\"Input array shape:\", ulch_train[rand_int][\"audio\"][\"array\"].shape)\n",
        "print(\"Sampling rate:\", ulch_train[rand_int][\"audio\"][\"sampling_rate\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9teZcSwOBJ4"
      },
      "source": [
        "Good! Everything looks fine - the data is a 1-dimensional array, the sampling rate always corresponds to 16kHz, and the target text is normalized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3Pbn5WvOYZF"
      },
      "source": [
        "Finally, we can leverage `Wav2Vec2BertProcessor` to process the data to the format expected by `Wav2Vec2BertForCTC` for training. To do so let's make use of Dataset's [`map(...)`](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=map#datasets.DatasetDict.map) function.\n",
        "\n",
        "First, we load and resample the audio data, simply by calling `batch[\"audio\"]`.\n",
        "Second, we extract the `input_features` from the loaded audio file. In our case, the `Wav2Vec2BertProcessor` creates a more complex representation as the raw waveform, known as [Log-Mel feature extraction](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum).\n",
        "Third, we encode the transcriptions to label ids.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDLYRSQog7e0"
      },
      "outputs": [],
      "source": [
        "import soundfile as sf\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94ATfGdfKf-f"
      },
      "outputs": [],
      "source": [
        "print(ulch_train.column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJY7I0XAwe9p"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(batch):\n",
        "    audio = batch[\"audio\"]\n",
        "    features = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"])\n",
        "\n",
        "    batch[\"input_features\"] = features.input_features[0]\n",
        "\n",
        "    batch[\"input_length\"] = len(batch[\"input_features\"])\n",
        "    if not batch[\"transcription\"] or batch[\"transcription\"].strip() == \"\":\n",
        "        raise ValueError(\"–û—à–∏–±–∫–∞: transcription –ø—É—Å—Ç–æ–π!\")\n",
        "\n",
        "    batch[\"labels\"] = processor(\n",
        "    text=batch[\"transcription\"],\n",
        "    return_tensors=\"pt\",\n",
        "    padding=\"longest\"\n",
        "    ).input_ids[0]\n",
        "\n",
        "    return batch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6Pg_WR3OGAP"
      },
      "source": [
        "Let's apply the data preparation function to all examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-mtkmtlKf-h"
      },
      "outputs": [],
      "source": [
        "ulch_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-np9xYK-wl8q"
      },
      "outputs": [],
      "source": [
        "ulch_train = ulch_train.map(prepare_dataset, remove_columns=ulch_train.column_names)\n",
        "#ulch_test = ulch_test.map(prepare_dataset, remove_columns=ulch_test.column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "928GlHFnKf-i"
      },
      "outputs": [],
      "source": [
        "ulch_test = ulch_test.map(prepare_dataset, remove_columns=ulch_test.column_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKcEWHvKI1by"
      },
      "source": [
        "**Note**: `datasets` automatically takes care of audio loading and resampling. If you wish to implement your own costumized data loading/sampling, feel free to just make use of the `\"path\"` column instead and disregard the `\"audio\"` column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZWDCCKqwcfS"
      },
      "source": [
        "Awesome, now we are ready to start training!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYlQkKVoRUos"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Slk403unUS91"
      },
      "source": [
        "### Set-up Trainer\n",
        "\n",
        "Let's start by defining the data collator. The code for the data collator was copied from [this example](https://github.com/huggingface/transformers/blob/7e61d56a45c19284cfda0cee8995fb552f6b1f4e/examples/pytorch/speech-recognition/run_speech_recognition_ctc.py#L219).\n",
        "\n",
        "Without going into too many details, in contrast to the common data collators, this data collator treats the `input_features` and `labels` differently and thus applies to separate padding functions on them. This is necessary because in speech input and output are of different modalities meaning that they should not be treated by the same padding function.\n",
        "Analogous to the common data collators, the padding tokens in the labels with `-100` so that those tokens are **not** taken into account when computing the loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tborvC9hx88e"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class DataCollatorCTCWithPadding:\n",
        "    processor: Wav2Vec2BertProcessor\n",
        "    padding: Union[bool, str] = True\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lenghts and need\n",
        "        # different padding methods\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "\n",
        "        batch = self.processor.pad(\n",
        "            input_features,\n",
        "            padding=self.padding,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        labels_batch = self.processor.pad(\n",
        "            labels=label_features,\n",
        "            padding=self.padding,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbQf5GuZyQ4_"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO-Zdj-5cxXp"
      },
      "source": [
        "Next, the evaluation metric is defined. As mentioned earlier, the\n",
        "predominant metric in ASR is the word error rate (WER), hence we will use it in this notebook as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5ZZ2du1Kf-l"
      },
      "outputs": [],
      "source": [
        "from evaluate import load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Xsux2gmyXso"
      },
      "outputs": [],
      "source": [
        "wer_metric = load(\"wer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjVUS58jbkxK"
      },
      "outputs": [],
      "source": [
        "cer_metric = load(\"cer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1qZU5p-deqB"
      },
      "source": [
        "The model will return a sequence of logit vectors:\n",
        "$\\mathbf{y}_1, \\ldots, \\mathbf{y}_m$ with $\\mathbf{y}_1 = f_{\\theta}(x_1, \\ldots, x_n)[0]$ and $n >> m$.\n",
        "\n",
        "A logit vector $\\mathbf{y}_1$ contains the log-odds for each word in the vocabulary we defined earlier, thus $\\text{len}(\\mathbf{y}_i) =$ `config.vocab_size`. We are interested in the most likely prediction of the model and thus take the `argmax(...)` of the logits. Also, we transform the encoded labels back to the original string by replacing `-100` with the `pad_token_id` and decoding the ids while making sure that consecutive tokens are **not** grouped to the same token in CTC style ${}^1$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XZ-kjweyTy_"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "    pred_logits = pred.predictions\n",
        "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
        "\n",
        "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
        "\n",
        "    pred_str = processor.batch_decode(pred_ids)\n",
        "    # we do not want to group tokens when computing the metrics\n",
        "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
        "\n",
        "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
        "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    #return {\"wer\": wer}\n",
        "    return {\"wer\": wer, \"cer\": cer}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xmgrx4bRwLIH"
      },
      "source": [
        "Now, we can load the pretrained checkpoint of [Wav2Vec2-XLS-R-300M](https://huggingface.co/facebook/wav2vec2-xls-r-300m). The tokenizer's `pad_token_id` must be to define the model's `pad_token_id` or in the case of `Wav2Vec2BertForCTC` also CTC's *blank token* ${}^2$. To save GPU memory, we enable PyTorch's [gradient checkpointing](https://pytorch.org/docs/stable/checkpoint.html) and also set the loss reduction to \"*mean*\".\n",
        "\n",
        "Since, we're only training a small subset of weights, the model is not prone to overfitting. Therefore, we make sure to disable all dropout layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7cqAWIayn6w"
      },
      "outputs": [],
      "source": [
        "from transformers import Wav2Vec2BertForCTC\n",
        "\n",
        "model = Wav2Vec2BertForCTC.from_pretrained(\n",
        "    \"facebook/w2v-bert-2.0\",\n",
        "    attention_dropout=0.0,\n",
        "    hidden_dropout=0.0,\n",
        "    feat_proj_dropout=0.0,\n",
        "    mask_time_prob=0.0,\n",
        "    layerdrop=0.0,\n",
        "    ctc_loss_reduction=\"mean\",\n",
        "    add_adapter=True,\n",
        "    pad_token_id=processor.tokenizer.pad_token_id,\n",
        "    vocab_size=len(processor.tokenizer),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lD4aGhQM0K-D"
      },
      "source": [
        "In a final step, we define all parameters related to training.\n",
        "To give more explanation on some of the parameters:\n",
        "- `group_by_length` makes training more efficient by grouping training samples of similar input length into one batch. This can significantly speed up training time by heavily reducing the overall number of useless padding tokens that are passed through the model\n",
        "- `learning_rate` was heuristically tuned until fine-tuning has become stable. Note that those parameters strongly depend on the Common Voice dataset and might be suboptimal for other speech datasets.\n",
        "\n",
        "For more explanations on other parameters, one can take a look at the [docs](https://huggingface.co/transformers/master/main_classes/trainer.html?highlight=trainer#trainingarguments)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbeKSV7uzGPP"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=repo_name,\n",
        "  group_by_length=False,\n",
        "  per_device_train_batch_size=8,\n",
        "  gradient_accumulation_steps=2,\n",
        "  eval_strategy=\"steps\",\n",
        "  num_train_epochs=1,\n",
        "  gradient_checkpointing=True,\n",
        "  bf16=True,\n",
        "  save_steps=368,\n",
        "  eval_steps=245,\n",
        "  logging_steps=245,\n",
        "  learning_rate=5e-5,\n",
        "  warmup_steps=500,\n",
        "  save_total_limit=2,\n",
        "  push_to_hub=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsW-WZcL1ZtN"
      },
      "source": [
        "Now, all instances can be passed to Trainer and we are ready to start training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rY7vBmFCPFgC"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    data_collator=data_collator,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=ulch_train,\n",
        "    eval_dataset=ulch_test,\n",
        "    processing_class=processor.feature_extractor,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoXBx1JAA0DX"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "${}^1$ To allow models to become independent of the speaker rate, in CTC, consecutive tokens that are identical are simply grouped as a single token. However, the encoded labels should not be grouped when decoding since they don't correspond to the predicted tokens of the model, which is why the `group_tokens=False` parameter has to be passed. If we wouldn't pass this parameter a word like `\"hello\"` would incorrectly be encoded, and decoded as `\"helo\"`.\n",
        "\n",
        "${}^2$ The blank token allows the model to predict a word, such as `\"hello\"` by forcing it to insert the blank token between the two l's. A CTC-conform prediction of `\"hello\"` of our model would be `[PAD] [PAD] \"h\" \"e\" \"e\" \"l\" \"l\" [PAD] \"l\" \"o\" \"o\" [PAD]`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpvZHM1xReIW"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-3oKSzZ1hGq"
      },
      "source": [
        "In case you want to use this google colab to fine-tune your model, you should make sure that your training doesn't stop due to inactivity. A simple hack to prevent this is to paste the following code into the console of this tab (*right mouse click -> inspect -> Console tab and insert code*)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYYAvgkW4P0m"
      },
      "source": [
        "```javascript\n",
        "function ConnectButton(){\n",
        "    console.log(\"Connect pushed\");\n",
        "    document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click()\n",
        "}\n",
        "setInterval(ConnectButton,60000);\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bGgLV2r0yvZ"
      },
      "source": [
        "Depending on what GPU was allocated to your google colab it might be possible that you are seeing an `\"out-of-memory\"` error here. In this case, it's probably best to reduce `per_device_train_batch_size` to 8 or even less and increase [`gradient_accumulation`](https://huggingface.co/transformers/master/main_classes/trainer.html#trainingarguments)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jty_i0BKKf-s"
      },
      "outputs": [],
      "source": [
        "ulch_train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIg392tJKf-s"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ya7WEy0pd13"
      },
      "source": [
        "You can now upload the result of the training to the ü§ó Hub, just execute this instruction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArG1Thf6NBWm"
      },
      "outputs": [],
      "source": [
        "trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmi1cX0fRBit"
      },
      "source": [
        "For more examples of how W2V-BERT can be fine-tuned, please take a look at the [official speech recognition examples](https://github.com/huggingface/transformers/tree/master/examples/pytorch/speech-recognition#examples)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8b8Qkoy3KyS"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R351I9IQp_9D"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCTC, Wav2Vec2BertProcessor\n",
        "\n",
        "model = AutoModelForCTC.from_pretrained(\"Ber5h/wav2vec-bert-2.0-ulch-try\").to(\"cuda\")\n",
        "processor = Wav2Vec2BertProcessor.from_pretrained(\"Ber5h/wav2vec-bert-2.0-ulch-try\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qD2v7KT6kurE"
      },
      "source": [
        "Let's process the audio, run a forward pass and predict the ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2Jf2cO7gKrp"
      },
      "outputs": [],
      "source": [
        "input_dict = ulch_test[0]\n",
        "\n",
        "logits = model(torch.tensor(input_dict[\"input_features\"]).to(\"cuda\").unsqueeze(0)).logits\n",
        "\n",
        "pred_ids = torch.argmax(logits, dim=-1)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiyKkRlpzAHO"
      },
      "source": [
        "\n",
        "Finally, we can decode the example from the predicted tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1L6lJFi2zAHO"
      },
      "outputs": [],
      "source": [
        "processor.decode(pred_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCjw3-e_vFO8"
      },
      "source": [
        "And compare it to the reference transcription:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yz8BmIAu8do"
      },
      "outputs": [],
      "source": [
        "processor.decode(input_dict[\"labels\"]).lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIAMJQlUjG1K"
      },
      "outputs": [],
      "source": [
        "len(ulch_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wBIB3d8Q6DN"
      },
      "outputs": [],
      "source": [
        "predictions = []\n",
        "references = []\n",
        "for i in range(len(ulch_test)):\n",
        "    input_dict = ulch_test[i]\n",
        "    logits = model(torch.tensor(input_dict[\"input_features\"]).to(\"cuda\").unsqueeze(0)).logits\n",
        "    pred_ids = torch.argmax(logits, dim=-1)[0]\n",
        "    print(processor.decode(pred_ids), processor.decode(input_dict[\"labels\"]).lower())\n",
        "\n",
        "\n",
        "wer = wer_metric.compute(predictions=predictions, references=references)\n",
        "cer = cer_metric.compute(predictions=predictions, references=references)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Dw5cfYolwl7"
      },
      "outputs": [],
      "source": [
        "wer, cer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EZb7MKrlDA6"
      },
      "outputs": [],
      "source": [
        "pred_real = pd.DataFrame()\n",
        "pred_real['pred'] = predictions\n",
        "pred_real['real'] = references\n",
        "pred_real.to_csv('pred_real.csv',index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}